# Twitter Analysis Project

Brenden Smith

## Project Overview

For this project, I chose to explore the tweets and timelines of Twitter. To do this I chose the sentiment analysis and parts of speech tagging functions from the NLTK package. With the sentiment analysis, I hoped to discover whether the accumulative sentiments and polarization of numerous tweets would differ depending on the subject the user entered. For instance, would tweets about hot-button political issues have stronger sentiments and more variance in sentiments than mundane or cheery topics? Secondly, with the parts of speech tagging function, I hoped to explore individual timelines to understand the grammar structure particular users implement with their tweets.

## Implementation

For both functions, I first needed to acquire unique keys and secrets and create an app within Twitter in order to get access to the trove of tweets in the Twitter database. Once I was authenticated, the first step in building the tweet sentiment analysis function was pulling the tweets from Twitter. I utilized the 30-day search feature from the tweepy package to acquire the most recent tweets (upon running the function) within the last 30 days on a particular subject. That subject would be entered by the user when calling the function and given to the API search to pull tweets containing the word or phrase entered. I excluded retweets to focus on only original tweets from accounts. By running the text of the tweet through the sentiment analyzer from NLTK, I would acquire the score. This includes the compound score, which is an aggregate of the positive, neutral, and negative components estimated by the analyzer. Some tweets registered a compound score of 0.0 with the analyzer (ex. those in a non-English language) and I thus filtered those out to only further deal with scores greater or less than 0. For the amount of tweets the user was interested in seeing (entered through calling function), I created a list that included the compound scores for all the tweets. From this list, I was able to calculate the sum of the compound scores, the average compound score, the sum of the absolute value of the compound scores, the average of the absolute value of the compound scores, and the standard deviations of the compound scores. Those figures were then printed out.
![alt text](sentiment_topics.png "Title")

With the tweet grammar analyzer, I also needed the keys and secrets to access the API for Twitter. Again using the resources provided by the tweepy package, I implemented the user timeline function to access the most recent tweets of a particular Twitter account. The program user would enter the Twitter handle (minus the “@” symbol) they want to inspect and then the maximum number of tweets they are interested in from the account. Since the function once again filters out retweets, I say maximum number of tweets because the function can only pull the grand number of tweets from the account – original tweet and retweets – and then filters out retweets afterwards. It cannot pull just original tweets. Therefore the number of tweets that are pulled are almost always greater than the function ultimately prints out, but the user can only determine how many tweets to pull. After acquiring the text of the tweet, I tokenized and tagged the contents of the text with the NLTK package, which assigns the parts of speech to the different words, and then assigned the items to a list. By traversing the list with a “for loop,” I created a dictionary where the keys were the part of speech and the values were the number of times they appeared in the tweet. The function accumulates these totals across the number of tweets the user is interested in and prints out the final counts for the parts of speech in a sorted list.

## Results

For analyzing sentiments in tweets, I passed through a number of different topics with the maximum number of tweets to be returned at 50. The initial takeaway was that the standard deviations didn’t seem to fluctuate that much, regardless of the salience of the topic. However, the average compound scores did seem to back up my hypothesis. Contentious political individuals and topics, such as President Joe Biden (“Biden”) and the debate over abortion (“abortion) resulted in compound scores closest to 0, meaning both sides of the issues cancelled each out quite a bit. Topics representing mundane or ordinary things (ie. “water” and “pizza”) registered positive compound scores, but not overwhelmingly so. This could be due to the fact that no one feels strongly about them or that if a tweet containing the word does have strong positive or negative connotations, it is about something else in the tweet, not the word itself. Thus, much like the political tweets, the strong emotions may balance each other out. Additionally, I included topics that are considered universally positive, like kittens and the recently-released film Dune, or universally negative, for example, MLB commissioner Rob Manfred (“Manfred”). The former topics registered the highest average compound score and the latter registered, by far, the lowest average compound score. The numbers supporting these conclusions can be found in the following table and box plot.

With the grammar analyzer I was able to discern what parts of speech were most popular for certain Twitter accounts and make comparisons with different accounts. For example, with this function I analyzed the last five original tweets from both The New York Times newspaper (@nytimes) and Babson College (@babson). As can be seen in the accompanying chart, both accounts use plural nouns (NNS), prepositions (IN), moderate-sized adjectives (JJ), and proper nouns (NNP) frequently. However, Babson used proper nouns over twice as much as The Times (19 vs. 9). Additionally, both accounts used parts of speech that the other account did not use. These similarities could occur because those types of parts of speech form the core of speech universally. The differences could be explained by stylistic and objective contrasts between the two accounts. Such analysis could be extrapolated to accounts that are even further apart in message style and substance and over a larger number of tweets.

# Reflection

After completing the process of understanding the possibilities with this project, the coding to undertake those possibilities, and the analysis to form conclusions on the topics, I’ve learned quite a bit about the usability of the Twitter database, different ways to use Python in the real word, and information about topics I was interested in. From a process standpoint, the tweepy package contains many different functions and I was able to learn the ones I executed quite well. In the future, I’d like to see them build functionality for pulling original tweets, however. Additionally, I found the sentiment analysis tool especially useful and easy to understand, which will benefit me if I choose to use it again in the future. Going forward, I would like to learn more about tables and graphing functionality in Python, if such code exists, in order to house my analysis within Python instead of printing the data and analyzing it in Excel. Thanks to the skills I learned in this project, I will be better prepared when completing my final project within my group as we are also planning on analyzing/retrieving data from Twitter. 